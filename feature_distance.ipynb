{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-b1129a2099c1>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-56ac5b223d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shravya/data_mining/assignment/hw1/spark-2.3.1-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/shravya/data_mining/assignment/hw1/spark-2.3.1-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 308\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-b1129a2099c1>:5 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id', 'business_reviewed_count', 'average_stars']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Arizona_10users.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one_user_df = df.first()\n",
    "# one_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userId = one_user_df[\"user_id\"]\n",
    "# type(one_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, business_reviewed_count: int, average_stars: double]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five = df.limit(10)\n",
    "first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_five = first_five.rdd.filter(lambda x: x != one_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = first_five.rdd.map(lambda x: (x[0],x[1],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|               _2|               _3|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|               10|               10|\n",
      "|   mean|             14.3|            3.412|\n",
      "| stddev|9.006787563955431|0.977716159674621|\n",
      "|    min|                6|             2.29|\n",
      "|    max|               31|              4.9|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf = new.toDF()\n",
    "newDf['_2','_3'].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_rdd = newDf.rdd.map(lambda x: (x[0],((x[1] - 14.3)/9.01),((x[2] - 3.41)/0.98)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'-55DgUo52I3zW9RxkZ-EAQ', 1.2985571587125415, 1.1734693877551015),\n",
       " (u'-inp099-1gsJF3KPaS-mbg', 0.41065482796892333, 0.2653061224489794),\n",
       " (u'0JaDPuKP812uXA6VIDMPZA', -0.921198668146504, -1.142857142857143),\n",
       " (u'1M5EuCw_7tJjJVmmnLmR9g', -0.921198668146504, -0.5306122448979592),\n",
       " (u'1Q-tjPYDBaFZJouj8Ynl2g', 0.632630410654828, 1.5204081632653064),\n",
       " (u'1jtr65gmv4A3uqCDaaXcxQ', -0.921198668146504, -0.41836734693877564),\n",
       " (u'2OtKt_7AZkfPXioU317R7g', 1.8534961154273029, 1.2959183673469383),\n",
       " (u'2zqSXXGVGmTWZ86mAq-69w', -0.6992230854605994, -1.1020408163265307),\n",
       " (u'3UA89mI2pN2KphXN3QXPIQ', -0.3662597114317426, -0.5408163265306125),\n",
       " (u'3g-2-nE22LxldS3Rx5IzNg', -0.3662597114317426, -0.5000000000000002)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_user = normalized_rdd.first()\n",
    "filtered_norm_users = normalized_rdd.filter(lambda x: x != first_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'-55DgUo52I3zW9RxkZ-EAQ', 1.2985571587125415, 1.1734693877551015)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(user):\n",
    "    user = list(user)\n",
    "    num = 0\n",
    "    den1 = 0\n",
    "    den2 = 0\n",
    "    for i in range(1,len(user)):\n",
    "        num += user[i] * first_user[i]\n",
    "        den1 += user[i] * user[i]\n",
    "        den2 += first_user[i] * first_user[i]\n",
    "    den1 = math.sqrt(den1)\n",
    "    den2 = math.sqrt(den2)\n",
    "    cos_sim = num / float(den1 * den2)\n",
    "    return cos_sim\n",
    "\n",
    "corr = filtered_norm_users.map(lambda x : (x[0], x[1], x[2], cosine_sim(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'-inp099-1gsJF3KPaS-mbg',\n",
       "  0.41065482796892333,\n",
       "  0.2653061224489794,\n",
       "  0.9870286235232484),\n",
       " (u'0JaDPuKP812uXA6VIDMPZA',\n",
       "  -0.921198668146504,\n",
       "  -1.142857142857143,\n",
       "  -0.9876165388731567),\n",
       " (u'1M5EuCw_7tJjJVmmnLmR9g',\n",
       "  -0.921198668146504,\n",
       "  -0.5306122448979592,\n",
       "  -0.9775587683080643),\n",
       " (u'1Q-tjPYDBaFZJouj8Ynl2g',\n",
       "  0.632630410654828,\n",
       "  1.5204081632653064,\n",
       "  0.904045678062686),\n",
       " (u'1jtr65gmv4A3uqCDaaXcxQ',\n",
       "  -0.921198668146504,\n",
       "  -0.41836734693877564,\n",
       "  -0.9527792155191327),\n",
       " (u'2OtKt_7AZkfPXioU317R7g',\n",
       "  1.8534961154273029,\n",
       "  1.2959183673469383,\n",
       "  0.9922390243234007),\n",
       " (u'2zqSXXGVGmTWZ86mAq-69w',\n",
       "  -0.6992230854605994,\n",
       "  -1.1020408163265307,\n",
       "  -0.9636190150972948),\n",
       " (u'3UA89mI2pN2KphXN3QXPIQ',\n",
       "  -0.3662597114317426,\n",
       "  -0.5408163265306125,\n",
       "  -0.9711778070265059),\n",
       " (u'3g-2-nE22LxldS3Rx5IzNg',\n",
       "  -0.3662597114317426,\n",
       "  -0.5000000000000002,\n",
       "  -0.9793173099554092)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
